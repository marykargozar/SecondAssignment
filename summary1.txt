Data preprocessing involves preparing and cleaning text data so that machines can analyze it. Preprocessing puts data in a workable form and highlights features in the text that an algorithm can work with. Tokenization substitutes sensitive information with nonsensitive information, or a token. There are several ways this can be done, including the following:
Tokenization. Tokenization is often used in payment transactions to protect credit card data. Stop word removal. Common words are removed from the text, so unique words that offer the most information about the text remain. Lemmatization and stemming. For example, the word "walking" would be reduced to its root form, or stem, "walk" to process. Lemmatization groups together different inflected versions of the same word. Words are tagged based on which part of speech they correspond to -- such as nouns, verbs or adjectives. Part-of-speech tagging.