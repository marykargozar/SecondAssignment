{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73be2392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\navid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\navid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from heapq import nlargest\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd41592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize text word frequency\n",
    "def summarize_text_frequency(text, context_window_limit):\n",
    "    \n",
    "    # divide text into sentences\n",
    "    sentences = sent_tokenize(text)  \n",
    "    # divide text into words\n",
    "    words = word_tokenize(text)  \n",
    "    words = [word.lower() for word in words if word.lower() not in stopwords.words('english') and word.isalnum()]\n",
    "# calculate word frequency\n",
    "    word_freq = FreqDist(words)  \n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        score = sum(word_freq[word] for word in word_tokenize(sentence.lower()) if word in word_freq)\n",
    "        sentence_scores[sentence] = score\n",
    "\n",
    "    num_sentences = context_window_limit // 20\n",
    "    top_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    summary = ' '.join(top_sentences)\n",
    "    summary_words = word_tokenize(summary)\n",
    "    if len(summary_words) > context_window_limit:\n",
    "        summary = ' '.join(summary_words[:context_window_limit]) \n",
    "\n",
    "    return summary\n",
    "\n",
    "# calculate similarity matrix for sentences\n",
    "def compute_similarity_matrix(sentences):\n",
    "    vectorizer = CountVectorizer().fit_transform(sentences)\n",
    "    return cosine_similarity(vectorizer)\n",
    "\n",
    "# summarize text using cosine similarity\n",
    "def summarize_text_cosine_similarity(text, context_window_limit):\n",
    "     # Split text into sentences\n",
    "    sentences = sent_tokenize(text) \n",
    "    length = 0\n",
    "    summary = \"\"\n",
    "\n",
    "    similarity_matrix = compute_similarity_matrix(sentences)  \n",
    "\n",
    "    while length < context_window_limit and length < len(sentences):\n",
    "        most_similar_index = similarity_matrix[length].argsort()[-1]\n",
    "        sentence = sentences[most_similar_index]\n",
    "        summary =summary + sentence + \" \"\n",
    "        length = length + len(word_tokenize(sentence))\n",
    "        # avoid repetition\n",
    "        similarity_matrix[:, most_similar_index] = 0  \n",
    "\n",
    "    return summary.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aeaa8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_summarization(text, context_window_limit, method=\"frequency\"):\n",
    "    sentences = sent_tokenize(text)\n",
    "    slice_size = max(1, len(sentences) // 5)\n",
    "    summaries = []\n",
    "    for start in range(0, len(sentences), slice_size):\n",
    "        end = min(start + slice_size, len(sentences))\n",
    "        slice_text = ' '.join(sentences[start:end])\n",
    "        summary = perform_summarization(slice_text, context_window_limit, method)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    final_summary = ' '.join(summaries)\n",
    "    while len(word_tokenize(final_summary)) > context_window_limit:\n",
    "        final_summary = perform_summarization(final_summary, context_window_limit, method)\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "576e2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_summarization(content, context_window_limit, method=\"frequency\"):\n",
    "    if method == \"frequency\":\n",
    "        return summarize_text_frequency(content, context_window_limit)\n",
    "    elif method == \"cosine\":\n",
    "        return summarize_text_cosine_similarity(content, context_window_limit)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown summarization method\")\n",
    "\n",
    "def process_documents(doc1, doc2, context_window_limit, method=\"frequency\"):\n",
    "    doc1_length = len(word_tokenize(doc1))\n",
    "    doc2_length = len(word_tokenize(doc2))\n",
    "    total_length = doc1_length + doc2_length\n",
    "    doc1_target_length = int((doc1_length / total_length) * context_window_limit)\n",
    "    doc2_target_length = context_window_limit - doc1_target_length\n",
    "\n",
    "    summary1 = hierarchical_summarization(doc1, doc1_target_length, method)\n",
    "    summary2 = hierarchical_summarization(doc2, doc2_target_length, method)\n",
    "\n",
    "    save_summary(\"summary1.txt\", summary1)\n",
    "    save_summary(\"summary2.txt\", summary2)\n",
    "\n",
    "    generate_query(summary1, summary2)\n",
    "\n",
    "def save_summary(filename, summary):\n",
    "    if summary:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(summary)\n",
    "\n",
    "def generate_query(summary1, summary2):\n",
    "    print(\"Generating query from summaries...\")\n",
    "    query = f\"\\nDocument 1 summary: {summary1}\\n\\nDocument 2 summary: {summary2}\"\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da2cae00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating query from summaries...\n",
      "\n",
      "Document 1 summary: Data preprocessing involves preparing and cleaning text data so that machines can analyze it. Preprocessing puts data in a workable form and highlights features in the text that an algorithm can work with. Tokenization substitutes sensitive information with nonsensitive information, or a token. There are several ways this can be done, including the following:\n",
      "Tokenization. Tokenization is often used in payment transactions to protect credit card data. Stop word removal. Common words are removed from the text, so unique words that offer the most information about the text remain. Lemmatization and stemming. For example, the word \"walking\" would be reduced to its root form, or stem, \"walk\" to process. Lemmatization groups together different inflected versions of the same word. Words are tagged based on which part of speech they correspond to -- such as nouns, verbs or adjectives. Part-of-speech tagging.\n",
      "\n",
      "Document 2 summary: As an illustration, when 1,500 senior business leaders in the United States in 2017 were asked about AI, only 17 percent said they were familiar with it. Most people are not very familiar with the concept of artificial intelligence (AI). They understood there was considerable potential for altering business processes, but were not clear how AI could be deployed within their own organizations. A number of them were not sure what it was or how it would affect their particular companies. It is a wide-ranging tool that enables people to rethink how we integrate information, analyze data, and use the resulting insights to improve decisionmaking. Despite its widespread lack of familiarity, AI is a technology that is transforming every walk of life. In this paper, we discuss novel applications in finance, national security, health care, criminal justice, transportation, and smart cities, and address issues such as data access problems, algorithmic bias, AI ethics and transparency, and legal liability for AI decisions. Our hope through this comprehensive overview is to explain AI to an audience of policymakers, opinion leaders, and interested observers, and demonstrate how AI already is\n",
      "    altering the world and raising important questions for society, the economy, and governance. In order to maximize AI benefits, we recommend nine steps for going forward: Encourage greater data access for researchers without compromising usersâ€™ personal privacy, invest more government funding in unclassified AI research, promote new models of digital education and AI workforce development so employees have the skills needed in the 21st-century economy, create a federal \n",
      "    AI advisory committee to make policy recommendations, engage with state and local officials so they enact effective policies, regulate broad AI principles rather than specific algorithms, take bias complaints seriously so AI does not replicate historic injustice, unfairness, or discrimination in data or algorithms, maintain mechanisms for human oversight and control, and penalize malicious AI behavior and promote cybersecurity. We contrast the regulatory approaches of the U.S. and European Union, and close by making a number of recommendations for getting the most out of AI while still protecting important human values.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    input_text1 = \"\"\"Data preprocessing involves preparing and cleaning text data so that machines can analyze it. Preprocessing puts data in a workable form and highlights features in the text that an algorithm can work with. There are several ways this can be done, including the following:\n",
    "Tokenization. Tokenization substitutes sensitive information with nonsensitive information, or a token. Tokenization is often used in payment transactions to protect credit card data.\n",
    "Stop word removal. Common words are removed from the text, so unique words that offer the most information about the text remain.\n",
    "Lemmatization and stemming. Lemmatization groups together different inflected versions of the same word. For example, the word \"walking\" would be reduced to its root form, or stem, \"walk\" to process.\n",
    "Part-of-speech tagging. Words are tagged based on which part of speech they correspond to -- such as nouns, verbs or adjectives.\n",
    "\"\"\"\n",
    "    input_text2 = \"\"\"Most people are not very familiar with the concept of artificial intelligence (AI). As an illustration, when 1,500 senior business leaders in the United States in 2017 were asked about AI, only 17 percent said they were familiar with it. A number of them were not sure what it was or how it would affect their particular companies. They understood there was considerable potential for altering business processes, but were not clear how AI could be deployed within their own organizations. Despite its widespread lack of familiarity, AI is a technology that is transforming every walk of life. It is a wide-ranging tool that enables people to rethink how we integrate information, analyze data, and use the resulting insights to improve decisionmaking. Our hope through this comprehensive overview is to explain AI to an audience of policymakers, opinion leaders, and interested observers, and demonstrate how AI already is\n",
    "    altering the world and raising important questions for society, the economy, and governance. In this paper, we discuss novel applications in finance, national security, health care, criminal justice, transportation, and smart cities, and address issues such as data access problems, algorithmic bias, AI ethics and transparency, and legal liability for AI decisions. We contrast the regulatory approaches of the U.S. and European Union, and close by making a number of recommendations for getting the most out of AI while still protecting important human values. In order to maximize AI benefits, we recommend nine steps for going forward: Encourage greater data access for researchers without compromising usersâ€™ personal privacy, invest more government funding in unclassified AI research, promote new models of digital education and AI workforce development so employees have the skills needed in the 21st-century economy, create a federal \n",
    "    AI advisory committee to make policy recommendations, engage with state and local officials so they enact effective policies, regulate broad AI principles rather than specific algorithms, take bias complaints seriously so AI does not replicate historic injustice, unfairness, or discrimination in data or algorithms, maintain mechanisms for human oversight and control, and penalize malicious AI behavior and promote cybersecurity.\"\"\"\n",
    "\n",
    "    context_window_limit = 4000\n",
    "\n",
    "    process_documents(input_text1, input_text2, context_window_limit, method=\"frequency\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
